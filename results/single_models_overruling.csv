model,accuracy,cost,avg_latency
Meta-Llama-3-8B-Instruct-Lite,0.8864583333333333,0.04245970000000001,0.4747979179024696
Meta-Llama-3.1-8B-Instruct-Turbo,0.9114583333333334,0.07988346000000004,0.4330138973891735
gemma-2-9b-it,0.8447916666666667,0.1320378000000001,0.43703160832325616
Qwen2-72B-Instruct,0.9625,0.39528089999999955,0.392049782226483
gemma-2-27b-it,0.953125,0.35451279999999974,0.38135658502578734
Meta-Llama-3.1-70B-Instruct-Turbo,0.946875,0.3905563199999996,0.49122175549467406
Meta-Llama-3.1-405B-Instruct-Turbo,0.95625,2.2191249999999982,0.8352155091861884
MoT-LLM Cascade 2-level,0.9145833333333333,0.23968620000000002,0.6790007432301839
CoE 2-level,0.9489583333333333,0.09155608,0.8724000563224157
AutoMix_T 2-level,0.9583333333333334,1.0472406399999996,11.798775059978167
AutoMix_P 2-level,0.915625,0.6554305800000028,13.995799606541793
FrugalGPT 2-level,0.9354166666666667,0.28416243999999946,0.7611290586491426
MoT-LLM Cascade 3-level,0.934375,0.2391348600000001,0.7028501316905021
CoE 3-level,0.9604166666666667,0.062384999999999996,0.9217719840506713
AutoMix_T 3-level,0.915625,0.9784236800000018,11.280391764144102
AutoMix_P 3-level,0.925,0.645666660000001,10.238246979316076
FrugalGPT 3-level,0.9614583333333333,1.6132310400000003,1.3926725255946317
