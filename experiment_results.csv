model,accuracy,cost,average_latency
MoT Cascade,0.72,0.014292,3.363049249649048
Ensemble Cascade,0.8,0.015756800000000005,5.055247249603272
meta-llama/Llama-3-70b-chat-hf,0.88,0.018261,1.1501483345031738
meta-llama/Llama-3-8b-chat-hf,0.56,0.0041662,0.8288498306274414
meta-llama/Meta-Llama-3-70B-Instruct-Turbo,0.88,0.018153,1.302619504928589
meta-llama/Meta-Llama-3-8B-Instruct-Turbo,0.48,0.0041826,0.8721352577209472
google/gemma-7b-it,0.16,0.005171999999999999,2.330547752380371
mistralai/Mistral-7B-Instruct-v0.3,0.48,0.005143800000000001,2.693152904510498
mistralai/Mistral-7B-Instruct-v0.1,0.0,0.005585999999999999,2.5790959548950196
allenai/OLMo-7B,0.04,0.0079818,9.66724986076355
Qwen/Qwen1.5-7B-Chat,0.56,0.004686199999999999,1.3875662326812743
togethercomputer/alpaca-7b,0.0,0.0046904,2.789027624130249
togethercomputer/RedPajama-INCITE-7B-Chat,0.0,0.008559000000000002,31.579831590652464
Undi95/Toppy-M-7B,0.0,0.005130000000000001,2.040228796005249
