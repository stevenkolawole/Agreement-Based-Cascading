model,accuracy,cost,avg_latency
Meta-Llama-3-8B-Instruct-Lite,0.667,0.07422280000000009,1.0942304670810699
Meta-Llama-3.1-8B-Instruct-Turbo,0.733,0.14566392000000022,1.5351802234649659
gemma-2-9b-it,0.8,0.25637069999999995,1.592508775472641
Qwen2-72B-Instruct,0.915,0.7749656999999995,2.750833398103714
gemma-2-27b-it,0.851,0.6829320000000001,2.9098322858810426
Meta-Llama-3.1-70B-Instruct-Turbo,0.933,0.68691216,5.570922607183457
Meta-Llama-3.1-405B-Instruct-Turbo,0.945,3.9199550000000007,2.397170131444931
FrugalGPT 2-level,0.857,0.4461687000000006,4.986049662590027
MoT-LLM Cascade 2-level,0.88,0.7263955000000005,5.1329909796714785
CoE 2-level,0.931,0.31298080000000017,5.05961573433876
AutoMix_T 2-level,0.934,1.5215417399999984,12.052537818193436
AutoMix_P 2-level,0.903,1.5059646800000017,11.918489821195603
MoT-LLM Cascade 3-level,0.901,0.8756582199999997,3.25752076792717
CoE 3-level,0.949,0.6111850000000002,4.508641304969788