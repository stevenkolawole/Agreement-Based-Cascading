model,accuracy,cost,avg_latency
Meta-Llama-3.1-8B-Instruct-Turbo,0.68,0.0068603399999999995,1.1435489749908447
Meta-Llama-3.1-70B-Instruct-Turbo,0.8,0.03229424,2.277695894241333
Meta-Llama-3.1-405B-Instruct-Turbo,0.96,0.183515,2.28009370803833
Meta-Llama-3-8B-Instruct-Turbo,0.6,0.0065305799999999985,0.7860587024688721
Meta-Llama-3-70B-Instruct-Turbo,0.8,0.031616639999999994,1.266809310913086
Meta-Llama-3-8B-Instruct-Lite,0.52,0.0039155,2.3698900032043455
Meta-Llama-3-70B-Instruct-Lite,0.84,0.019522620000000004,5.295680046081543
Llama-3-8b-chat-hf,0.6,0.0071736,0.9679939937591553
Llama-3-70b-chat-hf,0.8,0.032220000000000006,1.0796125030517578
WizardLM-2-8x22B,0.84,0.008665599999999999,3.4075564098358155
gemma-2-27b-it,0.84,0.0324464,2.7890872287750246
gemma-2-9b-it,0.68,0.012086999999999999,1.6047025394439698
dbrx-instruct,0.6,0.007153,32.39983471870423
deepseek-llm-67b-chat,0.76,0.03662459999999999,5.204459190368652
gemma-2b-it,0.0,0.0039294,1.224425573348999
MythoMax-L2-13b,0.0,0.008854,1.6339687061309816
Llama-2-13b-chat-hf,0.28,0.016676999999999997,8.200276355743409
Mistral-7B-Instruct-v0.1,0.16,0.0088882,2.116787004470825
Mistral-7B-Instruct-v0.2,0.36,0.009529,2.4083881855010985
Mistral-7B-Instruct-v0.3,0.32,0.008570600000000001,1.2541171646118163
Mixtral-8x7B-Instruct-v0.1,0.6,0.0095812,2.420520477294922
Mixtral-8x22B-Instruct-v0.1,0.76,0.0091038,2.4408868885040285
Nous-Hermes-2-Mixtral-8x7B-DPO,0.04,0.008684399999999998,2.5530737686157225
Nous-Hermes-2-Yi-34B,0.56,0.0083596,2.814095220565796
Hermes-3-Llama-3.1-405B-Turbo,0.92,0.18239,3.1939889430999755
Qwen1.5-72B-Chat,0.72,0.036006300000000005,2.3826635551452635
Qwen1.5-110B-Chat,0.8,0.07301160000000001,2.897492694854736
Qwen2-72B-Instruct,0.84,0.03672,41.186892786026
StripedHyena-Nous-7B,0.0,0.0085822,1.766896324157715
