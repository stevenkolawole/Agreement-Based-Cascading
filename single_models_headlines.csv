model,accuracy,cost,avg_latency
Meta-Llama-3-8B-Instruct-Lite,0.606,0.02249730000000001,0.43545955848693846
Meta-Llama-3.1-8B-Instruct-Turbo,0.539,0.044085239999999984,0.45224124121665954
gemma-2-9b-it,0.514,0.07036710000000007,0.4147133896350861
Qwen2-72B-Instruct,0.636,0.20748959999999997,0.4149557838439941
gemma-2-27b-it,0.624,0.1902232,0.3471098763942719
Meta-Llama-3.1-70B-Instruct-Turbo,0.629,0.21557624000000045,0.544649866104126
Meta-Llama-3.1-405B-Instruct-Turbo,0.628,1.2249150000000002,0.7140952498912811
MoT-LLM Cascade 2-level,0.61,0.16747218,0.7599187359809876
CoE 2-level,0.618,0.08206968000000006,0.9799793252944946
AutoMix_T 2-level,0.62,0.8163870800000005,12.352327844142915
AutoMix_P 2-level,0.616,0.8028610200000021,10.300698348522186
FrugalGPT 2-level,0.601,0.16970862000000017,0.8314563243389129
MoT-LLM Cascade 3-level,0.604,0.15829931999999997,0.7450090646743774
CoE 3-level,0.628,0.15904999999999994,1.1219868535995483
AutoMix_T 3-level,0.617,3.3483313199999882,17.383525251865386
AutoMix_P 3-level,0.599,1.9488035400000034,19.5234663002491
FrugalGPT 3-level,0.632,0.7485748200000018,1.2406834959983826
